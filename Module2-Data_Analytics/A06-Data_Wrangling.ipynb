{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Data Wrangling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "This lesson meets the following learning objectives:\n",
    "- The ability to use Python for data wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Read through all of the text in this page. This assignment provides step-by-step training divided into numbered sections. The sections often contain embeded exectable code for demonstration.  Section headers with icons have special meanings:  \n",
    "\n",
    "- <i class=\"fas fa-puzzle-piece\"></i> The puzzle icon indicates that the section provides a practice exercise that must be completed.  Follow the instructions for the exercise and do what it asks.  Exercises must be turned in for credit.\n",
    "- <i class=\"fa fa-cogs\"></i> The cogs icon indicates that the section provides a task to perform.  Follow the instructions to complete the task.  Tasks are not turned in for credit but must be completed to continue progress.\n",
    "\n",
    "Review the list of items in the **Expected Outcomes** section to check that you feel comfortable with the material you just learned. If you do not, then take some time to re-review that material again. If after re-review you are not comfortable, do not feel confident or do not understand the material, please ask questions on Slack to help.\n",
    "\n",
    "Follow the instructions in the **What to turn in** section to turn in the exercises of the assginment for course credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "The purpose of this assignment is to build on Tidy data cleaning by using Python tools to \"massage\" or \"wrangle\" data into formats that are most useful for visualization and analytics.\n",
    "\n",
    "**What is data wrangling?**\n",
    "\n",
    "> Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. \n",
    "\n",
    "- [Data Wangling](https://en.wikipedia.org/wiki/Data_wrangling) *Wikipedia*\n",
    "\n",
    "Previously, we learned about Tidy rules for reformatting data.  Transforming data into a Tidy dataset is data wrangling.  We have also learned to how to correct data types, remove missing values and duplicates.  This lessons is, therefore, an opportunity to bring everything together.  Some of the material will be a review, but should help reinforce the concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-cogs\"></i> Notebook Setup\n",
    "As before, we import any needed packages at the top of our notebook. Let's import Numpy and Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "The first step in any data analytics task is import and exploration of data.  At this point, we have learned all of the steps we need to identify the data columns, their data types, recognize where we have missing values and recognize categorical and numeric variables in the data.   \n",
    "\n",
    "For this tutorial we will use a dataset named \"Abolone\" from the [University of California Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Abalone). The datafile is named `abalone.data` and is available in the data directory that accompanies this notebook.  The data has 10 \"attributes\" or variables. The following table describes these 10 variables, their types, and additional details.\n",
    "\n",
    "<table>\n",
    "    <tr><th>Name</th><th>Data Type</th><th>Metric</th><th>Description</th></tr>\n",
    "    <tr><td>Sample ID</td><td>integer</td><td></td><td>A unique number for each sample taken</td></tr>\n",
    "    <tr><td>Sex</td><td>nominal</td><td></td><td>M = 0, F = 1, and I = 2 (infant)</td></tr>\n",
    "\t<tr><td>Length</td><td>continuous</td><td>mm</td><td>Longest shell measurement</td></tr>\n",
    "\t<tr><td>Diameter</td><td>continuous</td><td>mm</td><td>perpendicular to length</td></tr>\n",
    "\t<tr><td>Height</td><td>continuous</td><td>mm</td><td>with meat in shell</td></tr>\n",
    "\t<tr><td>Whole weight</td><td>continuous</td><td>grams</td><td>whole abalone</td></tr>\n",
    "\t<tr><td>Shucked weight</td><td>continuous</td><td>grams</td><td>weight of meat</td></tr>\n",
    "\t<tr><td>Viscera weight</td><td>continuous</td><td>grams</td><td>gut weight (after bleeding)</td></tr>\n",
    "\t<tr><td>Shell weight</td><td>continuous</td><td>grams</td><td>after being dried</td></tr>\n",
    "\t<tr><td>Rings</td><td>integer</td><td></td><td>+1.5 gives the age in years</td></tr>\n",
    "</table>\n",
    "\n",
    "***Note:*** To demonstrate specific techniques of data wrangling, the dataset provided to you was altered: a sample ID column was added, the Sex column contains numeric IDs, and missing values were added as were duplicates.\n",
    "\n",
    "This data has no header information, so, we'll provide it when we import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = pd.read_csv('data/abalone.data', header = None)\n",
    "abalone.columns = ['Sample_ID','Sex', 'Length', 'Diameter', 'Height', \n",
    "          'Whole_weight', 'Shucked_weight', 'Viscera_weight', \n",
    "          'Shell_weight', 'Rings']\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Exploring Data Types\n",
    "First, let's explore how Pandas imported the data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than the first, second and last columns, all others were imported as `float64` which is a decimal value. The others were imported as an `integer`.  This looks correct for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of how big the data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can explore the distribution of numerical data using the `describe` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that even though the 'Sex' column was provided as a numeric value, it is actually meant to be categorical, with each sex represented as a unique number.  We can explore the categorical data using the `groupby` function, followed by the `size` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.groupby(by=['Sex']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Finding Missing Values\n",
    "Before proceeding with any analysis you should know the state of missing values in the dataset.  For most analytics missing values are not supported. Some tools will automatically ignore them but it may be easier, in some cases, to remove them.\n",
    "\n",
    "First, let's quantify how many missing values we have. The `isna` function will convert the data into `True` or `False` values: `True` if the value is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.isna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `sum` function to then identify how many missing values we have per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Inspecting Duplicates\n",
    "Sometimes we may or may not want duplicates in the data. This depends on the expectations of the experiments and the measurements taken. Sometimes duplicates may represent human error in data entry. So, let's look for duplicated data.  We have 4,184 rows, let's see how many unique values per column that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all of the columns we have fewer that 4,184 values.  For columns like 'Sex' we have 3 unique values, but these repeated values are expected.  The decimal values also have duplicates. The likelihood of seeing the exact same decimal values varies based on the distribution for the variable and the number of decimal values in the measurement.  The number of duplicated values does not seem unordinary.  However, the sample ID should be unique, yet we have 4,177 of them instead of 4,184. This implies we have duplicated samples in the data. \n",
    "\n",
    "We can identify then umber of duplicated 'Sample_ID' values are in the data by using the `duplicated` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.duplicated(subset='Sample_ID').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 7 duplicated rows. Now let's see which rows have duplicated samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone[abalone.duplicated(subset='Sample_ID', keep= False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the rows are exact duplicates, so this was probably human entry error. We need to remove the copies rows. We will do so in the **3.1 Filtering** section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleanup\n",
    "### 2.1 Correcting Data Types\n",
    "During the data exploration phase above, we noticed that the Sex column was provided as a number to represent the Sex category, and therefore, Pandas imported that column as a numeric value. We need to convert that to a categorical value, because the meaning of the column is not ordinal or numeric. We should covert it to a string object.\n",
    "\n",
    "We can do that with two functions that work on Series:  \n",
    "- `astype`  converts the type of data in the series. \n",
    "- `replace`  replaces values in the series.\n",
    "\n",
    "We'll use `astype` to convert the column to a string and `replace` to convert the numbers to more easily recognizable 'Male', 'Female' and 'Infant' strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert the column from an integer to a string.\n",
    "sex = abalone['Sex'].astype(str)\n",
    "\n",
    "# Second convert 0 to Male, 1 to Female, and 2 to Infant.\n",
    "sex = sex.replace('0', 'Male')\n",
    "sex = sex.replace('1', 'Female')\n",
    "sex = sex.replace('2', 'Infant')\n",
    "\n",
    "# Now replace the 'Sex' column of the dataframe with the new Series.\n",
    "abalone['Sex'] = sex\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the Sample ID column, despite that it is numeric should not be treated as a numeric column, so let's convert that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sample_ID to a string\n",
    "abalone['Sample_ID'] = abalone['Sample_ID'].astype(str)\n",
    "\n",
    "# Let's check out the datatypes to make sure they match our expectations:\n",
    "abalone.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Handling Missing Values\n",
    "As observed in section 2.2, we do indeed have missing values! Let's remove rows with missing values.  We can do so with the `dropna` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = abalone.dropna(axis=0)\n",
    "abalone.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the `axis` argument is set to 0 indicating we will remove rows with missing values. If we compare the `shape` of the dataframe now, with the shape when we first loaded it we will see that we have lost 2 rows with missing values.\n",
    "\n",
    "In addition to `dropna` you can also use the `fillna` and `replace` functions to rewrite the missing values to something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove duplicates we can use the [drop_duplicates](http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.drop_duplicates.html) function of Pandas.  If we explore the duplicated columns of section 2.3 above we'll see that the rows are the same for all columns.  In this case we can call `drop_duplicates` with no arguments.  However, let's assume we can't guarantee that each column is the same,  but we do want to remove duplicated samples.  We can do this by using the `subset` argument of the `drop_duplicates` function. We don't want to drop all duplicates, we need to keep one set. Therefore, we'll use the `keep` argument to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = abalone.drop_duplicates(['Sample_ID'], keep='first')\n",
    "abalone.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the `keep` argument will default to `first` so we don't need to provide it, but including it makes the code more clear.  We have now dropped all duplicated rows and we have 4,177 valid rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reshaping Data\n",
    "Data reshaping is about altering the way data is housed in the data frames of Pandas. It includes filtering of rows, merging data frames, concatenating data frames, grouping, melting and pivoting. We have learned about all of these functions already. As a reminder, the following is a summary of what we've learned:\n",
    "\n",
    "**Subsetting by Column**:\n",
    "- *Indexing with column names*\n",
    "  - Purpose: Allows you to slice the dataframe using column index names.\n",
    "  - Introduced:  Pandas Part 1 Notebook\n",
    "  - Example:\n",
    "  ```python\n",
    "   # Get the columns: Sample_ID, Sex, Height and Rings\n",
    "   subset = abalone[['Sample_ID', 'Sex', 'Height', 'Rings']]\n",
    "  ```\n",
    "- *Indexing with the `loc` function*\n",
    "  - Purpose: Allows you to slice the dataframe using row and column index names.\n",
    "  - Introduced:  Pandas Part 1 Notebook\n",
    "  - Example:\n",
    "  ```python\n",
    "   # Get the columns: Sample_ID, Sex, Height and Rings\n",
    "   subset = abalone.loc[:,['Sample_ID', 'Sex', 'Height', 'Rings']]\n",
    "  ```    \n",
    "  \n",
    "**Filtering Rows**:\n",
    "- *Boolean Indexing*\n",
    "  - Purpose: to filter rows that match desired criteria\n",
    "  - Introduced:  Pandas Part 1 Notebook\n",
    "  - Example: \n",
    "  \n",
    "  ```python\n",
    "   # Finds all rows with sex of \"Male\" and the number of rings > 3.\n",
    "   matches = (abalone['Sex'] == 'Male') & (abalone['Rings'] > 3)\n",
    "   male = abalone[matches]\n",
    "\n",
    "   # Or more succinctly\n",
    "   male = abalone[(abalone['Sex'] == 'Male') & (abalone['Rings'] > 3)]\n",
    "  ```\n",
    "\n",
    "**Grouping Data**:\n",
    "- *`groupby` function*\n",
    "  - Purpose:  To group rows together by \"classes\" or values of data. Allows you to perform aggregate functions, such as calculating means, summations, sizes, etc. You can create new data frames with aggregated values.\n",
    "  - Introduced:  Pandas Part 2 Notebook. \n",
    "  - Example:\n",
    "  ```python\n",
    "  # Calculate the mean column value by each sex:\n",
    "  abalone.groupby(by=\"Sex\").mean()\n",
    "  ```\n",
    "  \n",
    "**Merging DataFrames**:\n",
    "- *`concat` function*\n",
    "  - Purpose: To combine two dataframes.  Depending if the columns and row indexes are the same determines how the data frames are combined.\n",
    "  - Introduced:  Pandas Part 2 Notebook.\n",
    "\n",
    "**Melting**:\n",
    "- *`melt` function*\n",
    "  - Purpose:  Handles the case where categorical observations are stored in the header labels (i.e. violates Tidy rules).  It moves the header names into a new column and matches the corresponding values.\n",
    "  - Introduced:  Tidy Part 1 Notebook.\n",
    "\n",
    "**Pivoting**:\n",
    "- *`pivot` and `pivot_table` functions*\n",
    "  - Purpose: The opposite of `melt`. Uses unique values from one more columns to create new columns.\n",
    "  - Intorduced: Tidy Part 1 Notebook.\n",
    "  \n",
    "You can use any of these functions/techniques to reshape the data to meet Tidy standards and appropriate for the analytic or visualization you want to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <i class=\"fas fa-puzzle-piece\"></i> Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the apple data found in the `../data/apple_data.txt` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data using the approach we've used thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidy the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Outcomes\n",
    "At this point, you should feel comfortable with the following:\n",
    "- How to begin exploration of any dataset prior to analysis\n",
    "- Cleaning data and wrangling it into Tidy format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Turn in?\n",
    "Be sure to **commit** and **push** your changes to this notebook.  All practice exercises should be completed.  Once completed, send a **Slack message** to the instructor indicating you have completed this assignment. The instructor will verify all work is completed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
