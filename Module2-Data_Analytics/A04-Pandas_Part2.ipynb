{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Pandas Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "This lesson meets the following learning objectives:\n",
    "\n",
    "- The ability to use Python data structures provided in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Read through all of the text in this page. This assignment provides step-by-step training divided into numbered sections. The sections often contain embeded exectable code for demonstration.  Section headers with icons have special meanings:  \n",
    "\n",
    "- <i class=\"fas fa-puzzle-piece\"></i> The puzzle icon indicates that the section provides a practice exercise that must be completed.  Follow the instructions for the exercise and do what it asks.  Exercises must be turned in for credit.\n",
    "- <i class=\"fa fa-cogs\"></i> The cogs icon indicates that the section provides a task to perform.  Follow the instructions to complete the task.  Tasks are not turned in for credit but must be completed to continue progress.\n",
    "\n",
    "Review the list of items in the **Expected Outcomes** section to check that you feel comfortable with the material you just learned. If you do not, then take some time to re-review that material again. If after re-review you are not comfortable, do not feel confident or do not understand the material, please ask questions on Slack to help.\n",
    "\n",
    "Follow the instructions in the **What to turn in** section to turn in the exercises of the assginment for course credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-cogs\"></i>  Read the Tidy Data Paper\n",
    "Before proceeding, read the official Tidy Data paper:\n",
    "\n",
    "Wickham, H. (2014). [Tidy Data](https://www.jstatsoft.org/article/view/v059i10). Journal of Statistical Software, 59(10), 1â€“23.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-cogs\"></i>  Notebook Setup\n",
    "\n",
    "First, we need to import the pandas library (and Numpy library too).  All packages are imported at the top of the notebook. Execute the code in the following cell to get started with this notebook (type Ctrl+Enter in the cell below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy and pandas usage are often intertwined.\n",
    "# These abbreviations are ubiquitiously used.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will use some of the data objects created in the previous Pandas Part 1 tutorial. Specifically these objects.\n",
    "\n",
    "Recreate:\n",
    "+ `df`:  a generic data frame containing two columns named \"alpha\" and \"beta\".\n",
    "+ `iris_df`:  a data frame containing the imported iris dataset.\n",
    "\n",
    "First, let's create the `df` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {'alpha': [0, 1, 2, 3, 4],\n",
    "     'beta': ['a', 'b', 'c', 'd', 'e']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read in the iris data.  It should be in a `data` directory inside the same directory as this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, execute the following to view the `df` data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's review the `iris_df` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting values in a DataFrame object\n",
    "\n",
    "We often want to change or assign data at specific rows, columns, indexes or slices.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Inserting a New Column\n",
    "\n",
    "A new column can be added by using a new label as an index to an existing DataFrame object, and assigning to it, a new Series object as the values. Let's add a new column to the `df` object. The column will be named `gamma` and consist of a Series containing 5 numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new column\n",
    "new_series = pd.Series([4, 3, 2, 1, 0])\n",
    "df['gamma'] = new_series\n",
    "\n",
    "# Now print the data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can use a numpy array instead of a Seris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gamma'] = np.array([4, 3, 2, 1, 0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Numpy, the array of values provided must have the same number of values as there are rows in the data frame. Observe the effect if the list is too short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gamma'] = np.array([4, 3, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if a Series is used, NaN's are added to indicate missing values.  Let's add a new \"epsilon\" column that is **shorter** than the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['epsilon'] = pd.Series([1, 2, 3])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, observe the effect if a `pd.Series` object is **longer** than the other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['theta'] = pd.Series([0, 1, 3, 4, 5, 6])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the Series that were beyond the length of the data frame were excluded.\n",
    "\n",
    "It is also possible to overwrite existing columns using the same approach. Here we'll replace gamma with a series of 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gamma'] = pd.Series([1, 1, 1, 1, 1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like DataFrames, Series objects have indexes. When we add them, Pandas adds and aligns their values by their indexes. By default these are integer indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Create a copy of the `df` dataframe.\n",
    "+ Add a new column named \"delta\" to the copy that consists of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data\n",
    "\n",
    "As shown in the previous section, missing values are represented as 'NaN' in the table display. You can test for missing values or add missing values using the Numpy `np.nan` value: \n",
    "\n",
    "> pandas primarily uses the value `np.nan` to represent missing data. It is by default not included in computations. See the [Missing Data section](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#missing-data).\n",
    "\n",
    "The choice of what to do with missing data is specific to the analysis you are performing. Two common approaches are to drop rows (or columns) with missing data, or to impute (fill) the empty cells. To explore this, we will need a dataframe with missing values.  We introduced missing values into the `df` data frame when we added a new column using a Series object that was too short. As a reminder, let's look again at the `df` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the missing values in the \"epsilon\" column.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dropping Rows with Missing Values\n",
    "We can remove all rows with at least one missing value using the `dropna` function.  This is a member function of a DataFrame object so we can all it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: By default, `dropna()` (and a fair number of other functions) do not modify the data frame 'in place', rather they return a modified copy.  Because we did not store this modified data frame in another variable in the code above, Python simply prints it.  Observe that if we print the `df` object it remains intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to have the data frame change \"in place\", then you then have two choices:\n",
    "1. Use the `dropna` function's `inplace=True` argument.\n",
    "2. Assign the result, using the same name.\n",
    "    `df = df.dropna()`\n",
    "\n",
    "Let's save the dataframe with rows removed into a new data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with missing values.\n",
    "df_nomissing = df.dropna()\n",
    "# Print the contents of our new data frame.\n",
    "df_nomissing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Filling Missing Values\n",
    "\n",
    "In some cases, setting missing values to some other value (such as 0) may be appropriate.  This can be accomplished using the `df.fillna()` function and passing in the desired value. Below we'll fill missing values with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `fillna` function also does not replace the value \"in place\". If we want to save the dataframe with missing values replaced as 0's we must also save it to a new variable or use the `inplace` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that without the inplace argument the original data frame is unchanged.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.<i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "\n",
    "+ Create two new copies of the `df` dataframe:\n",
    "+ Add a new column to both that has missing values.\n",
    "+ In one copy, replace missing values with a value of your choice.\n",
    "+ In the other copy, drop rows with `NaN` values.\n",
    "+ Print both arrays to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Operations\n",
    "\n",
    "Pandas provides functions that \"operate\" or act on on rows or columns. Some of these include calculating the mean, covariance, correlation, percent change, etc.\n",
    "\n",
    "### 3.1 Mathematical Operations\n",
    "\n",
    "To explore these operations, Let's review the `iris_df` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to demonstrate the use of operation lets caculate the mean of each column.  The `df.mean()` function provides this for each numerical column of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can call `help()` on an existing object, or its abstract form.\n",
    "\n",
    "```python\n",
    "help(pd.DataFrame.mean)\n",
    "# Should be mostly the same as:\n",
    "help(iris_df.mean)\n",
    "```\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the help instructions returned by the previous line of code, we can calculate the mean of the rows by providing the `axis` argument. Recall that axis 0 refers to rows and 1 is columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte the mean of each row, and use the head function to only show the\n",
    "# top 5 rows.\n",
    "iris_df.mean(1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a complete listing of the mathematical functions available with DataFrame objects here:\n",
    "+ [Binary Operator Functions](https://pandas.pydata.org/docs/reference/frame.html#binary-operator-functions)\n",
    "+ [Computations and Descriptiont Statistics](https://pandas.pydata.org/docs/reference/frame.html#computations-descriptive-stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "View the [Computational tools](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html) and [statistical methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#method-summary) documentation.\n",
    "Using the list of operational functions choose five functions to use with the iris data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Apply\n",
    "\n",
    "Another common operation is the ability to \"apply\" another function to a set of rows or columns (or a subset created from slicing). To efficiently apply functions in this way there is the `df.apply()` function.  The list of arguments for the `df.apply()` function are shown below \n",
    "\n",
    "```python\n",
    "apply(func, axis=0, broadcast=None, raw=False, reduce=None, result_type=None, args=(), **kwds)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, you can view the full help documetnation for this function  by executing the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(df.apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the first argument passed to `apply` is the named of the function that should be \"applied\" to the data frame. As a simple exmaple, we can provide the `print` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can use `apply` to find the the data type of each column using the the built-in function `type` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we can use `apply` to calcualte the sum of each columns using the Numpy sum function (although there is a `sum` function built into DataFrames):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: by default `apply` performs the supplied function across columns.  To apply the function across rows, use the `axis` argument (see the help for `apply` above).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "Practice using `apply` on either the `df` or `iris_df` data frames using any two functions of your choice other than `print`, `type`, and `np.sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Occurances\n",
    "\n",
    "You can calculate the number of occurances for a set of values in a `pd.Series` object using the `value_counts` function.  These counts are similar to what you would see in a Histogram.  As an example, let's create a new series containing 10 random integers between 1 and 7. Then we will use `value_counts` to observe how many occurances there are of each integer.  First let's create the series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.random.randint(0, 7, size=10))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use `value_counts` to create our \"histogram\" or occurances of each integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "Ientify the number of occurances for each species (virginica, versicolor, setosa) in the `iris_df` object.  *Hint*: the `value_counts` function only works on a `pd.Series` object, not on the full data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Strings in the DataFrame\n",
    "\n",
    "Many times, data sets will have columns with string values that must be modified in some way, such as being split from one column into two, or joining columns into one. To demonstrate string methods, lets adjust the `df` object to add some strings.  As a reminder lets look at the df object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder lets find the dimensions of this data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create some lists of strings to add to our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['alpha', 'beta', 'gamma', 'delta']\n",
    "more_labels = ['foxtrot', 'whiskey', 'omega','epsilon','startrek']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the NumPy `np.random.choice` function to create two new columns for the `df` data frame using randomized values from the two lists of strings we just created.  Take some time to make sure you understand the following code, then execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iota'] = np.random.choice(labels, df.shape[0])\n",
    "df['kappa'] = np.random.choice(more_labels, df.shape[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder:** Creating new columns using NumPy arrays requires they are of the same length as the existing data frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 The `.str` Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we learn about working with Strings, let's first learn about the `str` object of Series objects.  If the Series object contains strings then you will have access to the `str` object. Remember that Pandas columns are Series objects.  So for any column that stores string values you can access the `str` object in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kappa'].str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the `str` object is of type  `pandas.core.strings.StringMethods`. This object provides an \"accessor\". Similar to the `loc` and `iloc` accessors of Pandas, the `str` accessor can be used to access characters of strings in the series. For example we can extract the first three characters of each string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kappa'].str[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `str` object also provides a variety of string-related functions such as `split` to split strings, `upper` to convert strings to uppercase, `lower` to convert to lowercase, and `len` to get the length of string. View the [String Methods documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#string-methods) to view examples of usage. And explore the [String Handling](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#string-handling) documentation to see a list of functions that are part of the `str` object. \n",
    "\n",
    "As an example, we can get the length of all strings in the series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kappa'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Combining Columns into One\n",
    "\n",
    "Combining strings from two different columns is easy!  For example, we can create a new column named `lambda` which contains the values of the `iota` and `kappa` columns by simply using the `+` operator.  We can include an underscore to separate the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lambda'] = df['iota'] + '_' + df['kappa']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Splitting Columns to Multiple\n",
    "To split a column into two or more strings we can use the `str` object's `split` function.  We just added a new column with two words separated by an underscore. Let's use the `split` function to separate the strings with that delimiter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lambda'].str.split('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that each row of the `lambda` column was split and a new list was created.  The split worked, but if our goal is to split a string and return it back to the data frame as two new columns we have a bit more work to do. \n",
    "\n",
    "Remember, in order to add new columns to a dataframe, we need two Series objects: one for each new column.  Before we show how to do this, let's look at the datatype that is returned when the split is performed. We can do this with the core `type` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['lambda'].str.split('_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the `split` function returns a Series object with each element a list. Fortunately, the `str` object of a Series can be used to unpack lists!  Observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1, split_2 = df['lambda'].str.split('_').str\n",
    "split_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the code above that the `str` object on the Series returned by the split can be used to unpack the elements into two new Series:  `split_1` and `split_2`. The `split_1` Series contains the first set of values and the `split_2` will be the second set of values.  \n",
    "\n",
    "To add two new columns to our dataframe we can unpack directly into two new columns for our dataframe rather than new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lamba column of the df into two new columns.\n",
    "df['split_1'], df['split_2'] = df['lambda'].str.split('_', 1).str\n",
    "\n",
    "# Print the data frame.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Create a list of five strings that represent dates in the form YYYY-MM-DD (e.g. 2020-02-20 for Feb 20th, 2020).\n",
    "+ Add this list of dates as a new column in the `df` dataframe.\n",
    "+ Now split the date into 3 new columns with one column representing the year, another the month and another they day.\n",
    "+ Combine the values from columns `alpha` and `beta` into a new column where the values are spearated with a colon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combining DataFrames\n",
    "\n",
    "Often it is useful to combine two or more dataframes whether by merging, joining, contenating or grouping.  We will explore each of these operations. \n",
    "\n",
    "***Note***: For a much more exahustive description of merging, joining and contatenation, and helpful diagrams, see the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html).\n",
    "\n",
    "To demonstrate concatenation, lets use the iris dataframe.  But first let's break it up into separate species-specific data frames, i.e. one dataframe per species.  We can use Fancy Indexing that we learned from the Pandas Part 1 tutorial to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_df = iris_df[iris_df['species'] == 'setosa']\n",
    "versicolor_df = iris_df[iris_df['species'] == 'versicolor']\n",
    "virginica_df = iris_df[iris_df['species'] == 'virginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's breifly peek at each dataframe to make sure we split them correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versicolor_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versicolor_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginica_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginica_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that each data frame is exactly 50 rows long and each one contains only the species specific data. Also notice that despite existing in new dataframes, the row indexes did not change.  The indexes of the `virginica_df` start with 100 rather than 1.  Therefore, as a reminder from the Pandas lesson Part 1, the following use of `loc` and `iloc` retreive the same rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginica_df.loc[100:104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginica_df.iloc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the rest of this section, let's assume that we imported these iris species data as separate data frames. So, let's reset the indexes to start at 0 for each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the indexes to start at 0 and do this inplace.\n",
    "# The drop argument prevents the function from adding the old\n",
    "# index as a new column of the dataframe\n",
    "virginica_df.reset_index(drop=True, inplace=True)\n",
    "versicolor_df.reset_index(drop=True, inplace=True)\n",
    "setosa_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the virginica_df to show the indexes are reset\n",
    "virginica_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Merging with `pd.concat`\n",
    "#### 5.1.1 Concatenating by rows\n",
    "\n",
    "Suppose now that we imported the species-specific iris data frames and we now want to merge them.  We can do this using the `pd.concat` function.  By default, the `pd.concat` function tries to merge data frames by combining the rows of each data frame one after the other.  This image from the [Pandas Merge,join and concatenate documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html) shows how three dataframes are combined (concatenated) into a single larger datafame:\n",
    "\n",
    "![rows concat](media/A04-merging_concat_basic.png)\n",
    "\n",
    "The first argument that the `pd.concat` function accepts is a list of dataframes that should be merged. Therefore, we must first create a list of the individual data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = [setosa_df, versicolor_df, virginica_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call `pd.concat` to merge the three data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes into a larger one\n",
    "new_iris_df = pd.concat(subsets)\n",
    "# Print the top few rows of the new data frame.\n",
    "new_iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that all the rows are there.\n",
    "new_iris_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the `new_iris_df` data frame is now back to 150 rows! We can also  test this by randomly sampling to see if we get a variety of species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iris_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our species-specific dataframes are now reunited!  Although, notice that the row indexes have duplicates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iris_df.loc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that indexing, we get 3 entries, one from each of the three species-specific data frames. When dataframes are combined, Pandas, will not reset the indexes so these rows maintained their original row names.  This could be confusing, especialy since the row names are numeric, so let's reset them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex the new dataframe\n",
    "new_iris_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Get the row with row name '4'. Now it should only be a single Series.\n",
    "new_iris_df.loc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: The concatenation shown above works because all three dataframes have the same columns with the same data type in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Create the following dataframe\n",
    "```Python\n",
    "df1 = pd.DataFrame(\n",
    "    {'alpha': [0, 1, 2, 3, 4],\n",
    "     'beta': ['a', 'b', 'c', 'd', 'e']}, index = ['I1', 'I2' ,'I3', 'I4', 'I5'])\n",
    "```\n",
    "+ Create a new dataframe named `df2` with column names \"delta\" and \"gamma\" that contins 5 rows with some index names that overlap with the `df1` dataframe and some that do not.\n",
    "+ Concatenate the two dataframes by rows and print the result.\n",
    "+ You should see the two have combined one after the other, but there should also be missing values added. \n",
    "+ Explain why there are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3. Concatenating by Columns\n",
    "What if we wanted to combine two or more dataframes by columns?  To explore this, consider the following dataframes from the Pandas documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'B': ['B2', 'B3', 'B6', 'B7'],\n",
    "                    'D': ['D2', 'D3', 'D6', 'D7'],\n",
    "                    'F': ['F2', 'F3', 'F6', 'F7']},\n",
    "                    index=[2, 3, 6, 7])\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                   index=[0, 1, 2, 3])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to merge these two dataframes?  Unlike the previous example we want to merge by concatenating columns rathar than rows.  To concatenate by colmns we can pass the `axis` argument. A value of `1` tells the funtion to concatenate by columns:\n",
    "\n",
    "```python\n",
    "  pd.concat([df1, df4], axis=1)\n",
    "```\n",
    "\n",
    "However, notice that each data frame has some column names in common, and some of the row names are in common, but both have some column and row names that are not shared. How will `pd.concat` handle this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df4], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the columns of the two tables are set side-by-side, even though some of the columns have the same name, and rows with the same index between the two data frames are merged.  Any columns that are missing in one data frame will have missing values added in.\n",
    "\n",
    "If we want to only include rows that have no missing values we can use the argument `join=\"inner\"` to automatically remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df4], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, an \"inner\" join removes rows whose index name is not present in both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.4. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "Using the same dataframes, df1 and df2, from the previous practice section:\n",
    "+ Concatenate the two by columns\n",
    "+ Add a \"delta\" column to `df1` and concatenate by columns such that there are 5 columns in the merged dataframe.\n",
    "+ Respond in writing to this question (add a new 'raw' cell to contain your answer). What will happen if using you had performed an inner join while concatenating?  \n",
    "+ Try the concatenation with the inner join to see if you are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.5. Concatenating by Columns and Rows\n",
    "If you want to merge two dataframes that share some, but not all column names and you want to maintain all of the rows from both, you can use concat as if you were combining by rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge by rows, but for dataframes that share column names.\n",
    "# We must set sort=False for this function.\n",
    "pd.concat([df1, df4], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we now only have columns `A`, `B`, `C`, `D` and `F`. The column `D` only appears once.  We also have all of the rows.  Missing values were inserted.\n",
    "\n",
    "**Important** If you want to combine two dataframes by rows and they have shared column names and you do not want the columns merged you must rename one of the column names to ensure they are all unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Append\n",
    "\n",
    "Adding rows to a dataframe can be done using the `append` function.  To demonstrate this, lets create a 4x6 data frame containing a collection of random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df = pd.DataFrame(np.random.random((4, 6)))\n",
    "rand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we want to append a new row to that data frame.  Let's create a new `pd.Series` object containing the same number of random numbers as there are columns in the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_row = pd.Series(np.random.random(6) * 100)\n",
    "rand_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we an call the `append` function.  Unlike the `pd.concat` function, the `append` function belongs to the DataFrame object.  The `append` function takes the new series as its first agument. We also pass the `ignore_index` argument set to `True`. This forces the append to re-number the row indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df.append(rand_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note*** the `append` function does not alter 'in place' the `rand_df` data frame. Instead it returns a new data frame with the row appended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Create a new 5x5 dataframe full of random numbers.\n",
    "+ Create a new 5x10 dataframe full of 1's.\n",
    "+ Append one to the other and print it.\n",
    "+ Append a single Series of zeros to the end of the appended dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Grouping\n",
    "\n",
    "#### 5.4.1 Grouping by a Single Column\n",
    "Grouping using the `groupby` function of a DataFrame object is a powerful sorting and subsetting tool of Pandas. The grouping performs three operations:  \n",
    "\n",
    "- Splitting the data frame\n",
    "- Applying a function\n",
    "- Combining the results\n",
    "\n",
    "\n",
    "This is best demonstrated with an example. Lets remind ourselves of the iris data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to know the mean, width and length of each tissue (sepal and petal) for each species. How would you do this? One might be inclined to write a Python function that iterates through the rows of the dataframe and calculates the stats for each species.\n",
    "\n",
    "There is an easier way to accomplish this using Panda's `groupby` function.   The `groupby` function allows us to \"collapse\" into groups, the rows of data by one or more columns.  Once collapsed into groups, we can then apply functions to the groups. First, start by grouping the iris dataset by species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.groupby('species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `groupBy` function returns a new object named `pandas.core.groupby.groupby.DataFrameGroupBy` or a `DataFrameGroupBy` object for short.  Lets re-run that command and save that object in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = iris_df.groupby('species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the `DataFrameGroupBy` object.  This object allows us to iterate over its \"groups\".  We can do so using a `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each group, print its type and contents.\n",
    "for group in groups:   \n",
    "  print(type(group))\n",
    "  print(group)\n",
    "    \n",
    "  # Let's stop the for loop after one iteration to save space in the Notebook.\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the groups are tubles.  Remember a tuple is like a list, but unlike a list,once it is created you can not change it. Tuples are represented using parentheses, `(` and `)`, rather than square brakets.  The first element of the tuple is the key. In the example printed above the key is `setosa`.  The second element of the tuple are the rows that belong to that group.   We learn from this, that the `DataFrameGroupBy` object is a list of tuples, where each tuple contains the rows of the dataframe that belong to the group.\n",
    "\n",
    "Now that we have our groups we can apply a function.  To get the mean we simply call `mean` on the groups object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result is a new data frame. \n",
    "\n",
    "There are a large variety of functions supported by the `DataFrameGroupBy` object. These include the `mean`, `var`, `std`, `min` and many more.  You can find the list of all functions available on the [GroupBy API documentation page](https://pandas.pydata.org/pandas-docs/version/0.23.1/api.html#groupby)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3 Grouping by a Multiple Columns\n",
    "\n",
    "It is possible to have `groupby` use more than one column to group. To demonstrate, let's suppose we took measurements of the iris sepal and petals at two different developmental stages: early and late flowering. We want to cacluate the mean for those two different periods.  In this case we would need to group by the `species` and by a new development stage column.  Let's create one by randomly assigning the development stage (of course in reality this data would be provided):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['dev_stage'] = np.random.choice(['early', 'late'], iris_df.shape[0])\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's group by both `species` and `dev_stage` and calcualte the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by species and developmental stage\n",
    "groups = iris_df.groupby(['species', 'dev_stage'])\n",
    "# Calculate the mean\n",
    "groups.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the mean organized by species and developmental stage!\n",
    "\n",
    "Notice in this dataframe that there are two row index names correponsding to the two group by columns. This is called a `MultiIndex`.  We will not explore MultiIndex however, you can learn more about it on the [MultiIndex/advanced indexing page](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.4. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "Demonstrate a `groupby`.\n",
    "\n",
    "+ Create a new column with the label \"region\" in the iris data frame. This column will indicates geographic regions of the US where measurments were taken. Values should include:  'Southeast', 'Northeast', 'Midwest', 'Southwest', 'Northwest'. Use these randomly.\n",
    "+ Use `groupby` to get a new data frame of means for each species in each region.\n",
    "+ Add a `dev_stage` column by randomly selecting from the values \"early\" and \"late\".\n",
    "+ Use `groupby` to get a new data frame of means for each species,in each region and each development stage.\n",
    "+ Use the `count` function (just like you used the `mean` function) to identify how many rows in the table belong to each combination of species + region + developmental stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Outcomes\n",
    "At this point, you should feel comfortable with the following:\n",
    "- Inserting new columns in a Pandas dataframe.\n",
    "- Working with missing data.\n",
    "- Performing mathmatics with dataframes.\n",
    "- Using the apply function in Pandas.\n",
    "- Working with strings in Pandas.\n",
    "- Concatenating columns and rows.\n",
    "- Appending data.\n",
    "- Grouping data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Turn in?\n",
    "Be sure to **commit** and **push** your changes to this notebook.  All practice exercises should be completed.  Once completed, send a **Slack message** to the instructor indicating you have completed this assignment. The instructor will verify all work is completed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
