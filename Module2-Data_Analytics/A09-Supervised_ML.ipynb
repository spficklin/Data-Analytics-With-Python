{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9: Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "This lesson meets the following learning objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Read through all of the text in this page. This assignment provides step-by-step training divided into numbered sections. The sections often contain embeded exectable code for demonstration.  Section headers with icons have special meanings:  \n",
    "\n",
    "- <i class=\"fas fa-puzzle-piece\"></i> The puzzle icon indicates that the section provides a practice exercise that must be completed.  Follow the instructions for the exercise and do what it asks.  Exercises must be turned in for credit.\n",
    "- <i class=\"fa fa-cogs\"></i> The cogs icon indicates that the section provides a task to perform.  Follow the instructions to complete the task.  Tasks are not turned in for credit but must be completed to continue progress.\n",
    "\n",
    "Review the list of items in the **Expected Outcomes** section to check that you feel comfortable with the material you just learned. If you do not, then take some time to re-review that material again. If after re-review you are not comfortable, do not feel confident or do not understand the material, please ask questions on Slack to help.\n",
    "\n",
    "Follow the instructions in the **What to turn in** section to turn in the exercises of the assginment for course credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "For this notebook we will learn about supervised machine learning using the **scikit-learn** (**sklearn**) package. skLearn is a Python library that provides machine clearning capabilities.  It is built on Numpy, SciPy and matplotlib.\n",
    "\n",
    "For machine learning there are three primary purposes:\n",
    "- Classification: to predict the outcome \"class\" to which a sample belongs.\n",
    "- Regression: predicting an outcome value on a continuous scale.\n",
    "- Clustering: automatic grouping of similar objects into outcome classes when those classes are unknown.\n",
    "\n",
    "You can read more at these links:\n",
    "\n",
    "- Classification: https://en.wikipedia.org/wiki/Statistical_classification\n",
    "- Prediction: https://en.wikipedia.org/wiki/Predictive_analytics\n",
    "- Clustering: https://en.wikipedia.org/wiki/Cluster_analysis\n",
    "\n",
    "For supervised machine learning, a training set of data is provided to a set of appropriate algorithms (i.e. for classification, regression or clustering). The training data set typically consists of a subset of all available data that includes both independent and dependent variables (i.e. **outcome**) measured across a variety of samples. This training data is provided to one or more algorithms which determine a **model** that can be used to classify or predict outcomes.  Typically, a set of data is set aside to test, or validate, the accuracy of the model.  \n",
    "\n",
    "For this notebook we will once again use the Iris dataset and we will use supervised machine learning to create a model for predicting species. Therefore, the outcome variable is `species` and all others (`sepal_width`, `sepal_length`, `petal_width`, `petal_length`) are the independent variables.  Therefore, this notebook will demonstrate a \"Classification\" example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-cogs\"></i> Notebook Setup\n",
    "As before, we import any needed packages at the top of our notebook. Let's import Numpy, Pandas, Seaborn, matplotlib and the sklearn machine learning libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Data Management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Summarize the dataset\n",
    "Just as we learned in the Data Wrangling notebook, we should always summarize the data. Execute the following commands to explore this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many samples do we have for our outcome variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.groupby('species').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Check for missing or duplicated data\n",
    "Just like we did in the Data Wrangling notebook, we want to check for missing values and duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[iris.duplicated(keep= False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Examine data distributions and check for outliers\n",
    "Some statistical methods make assumptions about the distribution of samples for each variable and the presence of outliers, so, we need to be aware of how our data appears and any potential outliers. Pandas dataframes have a very convenient `hist` function for printing histograms of all numeric columns. It is based on Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, Pandas dataframes also have a flexible `plot` function as well, that allows us to print other types of pltos, such as a boxplot. You can learn more about the plot function by viewing the [online documentation](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.plot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are outliers without our outcome groups. It turns out we can easily do this with a `grouby`.  The DataFrameGroupBy object, also has a `boxplot` function that makes this easy to view as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.groupby(by='species').boxplot(rot=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does indeed appear that we have outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Search for Collinearity\n",
    "It is important to identify collinearity prior to creating any machine learning model.  There are two types of collinearity:\n",
    "- Structurual: this occurs when we create new columns in our data that are derived from other columns. For example, a log transformation of one column.\n",
    "- Multicollinearity:  this can be natural in our data and occurs when one variable can be used to linearly predict another.  \n",
    "\n",
    "Why can collinearity be bad?  For regression models, collinearity can weaken the p-values because it makes the estimated co-efficients too sensitive to changes in the model.  For an example of the affect that collinarity can have with a regression model sse the [JMP Multicollinearity](https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-multiple-regression/multicollinearity.html) page.\n",
    "\n",
    "We can empirically check for collinarity using a simple pairwise scatterplot using Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should consider removing columns that are severly collinear. The more collinear, the more significant the impact on the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "After reviewing the data in sections 2.1, 2.2, 2.3 and 2.4 do you see any problems with this iris dataset? If so, please describe them in the practice notebook.  If not, simply indicate that there are no issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "After reviewing the data in sections 2.1, 2.2, 2.3 and 2.4 are there any columns that would make poor predictors of species? \n",
    "\n",
    "**Hint**: columns that are poor predictors are:\n",
    "+ those with too many missing values\n",
    "+ those with no difference in variation when grouped by the outcome class\n",
    "+ variables with high levels of collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare the Data\n",
    "The sklearn package expects that all indpendent variables are numerical and that independent and dependent variables are separated into different data objects. It also expects that data is in Numpy arrays (not Pandas data frames). First, let's separate the variables to only include those that are numeric. Then, we'll create a 2D Numpy array, named `X`, containing the indpendent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.loc[:,'sepal_length':'petal_width'].values\n",
    "\n",
    "# Show the contents of X by displaying the first 10 rows.\n",
    "X[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a new Numpy 1D array named `Y` that will house the dependent variable (species name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = iris['species'].values\n",
    "\n",
    "# Show the contents of X by displaying the first 10 elements.\n",
    "Y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we have no independent variables that are categorical. All are numeric. If we did have categorical data we must convert those to numeric values. sklearn requires that all data in `X` be numeric.  We have several options as defined in the [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features)  section of sklearn:\n",
    "1. Use the sklearn `processing.OrdinalEncoder` function \n",
    "   - It converts categorical data to ordered numbers. \n",
    "   - Use this only if the classes are also ordinal.\n",
    "2. Use the `processing.OneHotEncoder` function. \n",
    "   - It converts all but one class for a single variable to 0 and leaves one category of interest as 1.  \n",
    "   - Use this only if you are predicting only a single class of outcome.  \n",
    "3. Pivot the categorical column into multiple new binary columns. \n",
    "   - The values in the columns are 0 and 1 and indicate if the category applies to the sample row.  \n",
    "   - Use this if there are multiple classes for a single variable and they are not ordinal.  Unfortunately, this is **not** tidy. But it is required to handle multiple classes.\n",
    "\n",
    "As the iris data is all numeric we have no need for any of these options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Normalize the data\n",
    "Many machine learning algorithms expect that the quantitative columns are centered at 0 and scaled to unit variance.  See the [preprocessing documentation](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler) for sklearn.\n",
    "\n",
    "According to the sklearn documentation:\n",
    "\n",
    "> In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation... The function `scale` provides a quick and easy way to perform this operation on a single array-like dataset.\n",
    "\n",
    "> If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use `robust_scale`... [it uses] more robust estimates for the center and range of your data.\n",
    "\n",
    "\n",
    "We can, therefore, normalize the `X` Numpy array using the [preprocessing.scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale) or [preprocessing.robust_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html#sklearn.preprocessing.robust_scale) function of Sklearn. Choose the method most appropriate given the state of outliers in the data. As it is clear we have outliers in some of the columns of the iris data we should use the`robust_scale` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.robust_scale(X)\n",
    "\n",
    "# Show the contents of X by displaying the first 10 rows.\n",
    "# The values should be scaled between -1 to 1\n",
    "X[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  Split the data for testing and validation\n",
    "For supervised machine learning, you must provide a testing dataset to create a model.  The model is then used on another dataset to validate its accuracy.  If the dataset is large enough we split the original dataset into a test and a validation set.  We can do this using the sklearn `model_selection.train_test_split` function.  The function takes as input the 2D Numpy array of independent variables and the 1D Numpy array with the oucome variable.  It also takes the following arguments:\n",
    "\n",
    "- `test_size`: \"should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split.\" For example, to use 80% of the data for model construction and 20% for testing (i.e. validation) this argument should be set to 0.2.  The default is 0.25.\n",
    "- `random_state`: rows are randomly selected to include in the split. You can ensure the same random order by providing a seed.  This allows for reproducibility.\n",
    "\n",
    "Lets split the data with 20% used for testing and a random seed of 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "Xt, Xv, Yt, Yv = model_selection.train_test_split(X, Y, test_size=0.2, random_state=10)\n",
    "\n",
    "# Print the shapes of each dataset\n",
    "print(\"The sizes of the training independent and dependent datasets\")\n",
    "print(Xt.size)\n",
    "print(Yt.size)\n",
    "print(\"The sizes of the validation independent and dependent datasets\")\n",
    "print(Xv.size)\n",
    "print(Yv.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code above `Xt` and `Yt` become the \"training\" data.  and `Xv` and `Yv` become the \"testing\" data used for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform Supervised Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 K-Fold Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data separated into training and validation sets, we should establish a training strategy.  To avoid overfitting a model to the data, we should perform a K-fold cross-validation strategy.  This type of strategy will further divide our training data into <i>k</i> subsets. The model will be trained <i>k</i> times using <i>k</i>-1 subsets. The <i>k</i>th subset will be set aside for validation. But, for each of the <i>k</i> tests, a different subset will have its turn for validation.  An accuracy score is provided for each attempt.  You can evaulate the performance of a machine learning algorithm by exploring the distribution (mean and variance) of the <i>k</i> attempts (we will try this later in the notebook).\n",
    "\n",
    "![KFCV](./media/A08-cross_validation.png)\n",
    "\n",
    "<sup>Image from [TowardsDataScience.com Cross Validation](https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85) page.\n",
    "\n",
    "To establish a K-fold cross-validation strategy we use the `model_selection.KFold` function of sklearn.  It takes two important arguments:\n",
    "\n",
    "- `n_splits`: the number of subsets to split the data.\n",
    "- `random_state`: you can ensure the same random order in the subsets by providing a seed.  This allows for reproducibility.\n",
    "\n",
    "Lets create a K-fold strategy that splits the iris data into 10 subsets and a random seed of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Evaulate ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn provides a variety of supervised machine learning aglorithms. Here, we will evaluate six of them:\n",
    "\n",
    "1. Logistic Regression (LR)\n",
    "2. Linear Discriminant Analysis (LDA)\n",
    "3. K-Nearest Neighbors (KNN).\n",
    "4. Classification and Regression Trees (CART).\n",
    "5. Gaussian Naive Bayes (NB).\n",
    "6. Support Vector Machines (SVM).\n",
    "\n",
    "Each of these algorithms is provided by sklearn as an object. Notice, these were imported in Section 1 above: \n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "```\n",
    "\n",
    "To build a model, we must first create an object for the algorithm.  For example, to create an object for Logistic Regression we could use the following code:\n",
    "```python\n",
    "alg = LogisticRegression(solver='lbfgs', multi_class=\"auto\")\n",
    "```\n",
    "\n",
    "\n",
    "In the following sections, we will construct an object for each algorithm and use it to create a predictive model, but first, we need a way to store the results of all ML algorithms that we'll be using. To make this easy, we will store results from each method in a dictionary. The following code prepares a dictionary containing, as keys, the names of the 6 methods we will explore.  The value is an array of 10 zeros.  This is because we will be performing a 10-fold cross-validation and we replace the zeros with results of each of the 10 attempts per method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'LogisticRegression' : np.zeros(10),\n",
    "    'LinearDiscriminantAnalysis' : np.zeros(10),\n",
    "    'KNeighborsClassifier' : np.zeros(10),\n",
    "    'DecisionTreeClassifier' : np.zeros(10),\n",
    "    'GaussianNB' : np.zeros(10),\n",
    "    'SVC' : np.zeros(10)\n",
    "}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the algorithm object (in this example a LogisiticRegression object) we perform the model building and cross-validation in one step using the `model_selection.cross_val_score` funtion:\n",
    "\n",
    "```python\n",
    "model_selection.cross_val_score(alg, Xt, Yt, cv=kfold, scoring=\"accuracy\", error_score=np.nan)\n",
    "```\n",
    "Here we provide the algorithm object, the training data, validation data, k-fold object, the `scoring` accuracy argument and an `error_score` argument.  See the [online documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) for details about other arguments.  We can use the same `cross_val_score` function for any of the 6 algorithms.\n",
    "\n",
    "In the following sections, six supervised machine learning methods will be briefly introduced.  For brevity we will not explore these approaches too deeply. A brief summary is provided followed by links for additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Logistic Regression\n",
    "\n",
    "**Brief Summary** \n",
    "\n",
    "In statistics, the logistic model (or logit model) uses a logistic function to model a binary outcome (i.e. values of 0 and 1) and a set of one or more independent variables: [X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub>, ... X<sub>n</sub>]:\n",
    "\n",
    "\\begin{equation*}\n",
    "logit(p) = b_0 + b_1X_1 + b_2X_2 + b_2X_2 ... b_nX_n\n",
    "\\end{equation*}\n",
    "\n",
    "Its goal is to determine the set of coefficients, [b<sub>1</sub>, b<sub>2</sub>, b<sub>3</sub>, ... b<sub>n</sub>], that best fit the relationship between the dependent and independent variables.   Alternatively, it is the natural log of the odds ratio:\n",
    "\n",
    "\\begin{equation*}\n",
    "odds\\ ratio = \\frac{p}{1-p} = \\frac{probability\\ of\\ presence}{probability\\ of\\ absense}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "logit(p) = ln(\\frac{p}{1-p})\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "The following image from Wikipedia shows an example where the dependent variable of *\"passing an exam\"* with a single independent variable of *\"hours studied\"*. \n",
    "\n",
    "<img src=\"./media/A08-Exam_pass_logistic_curve.jpeg\" style=\"height:300px\">\n",
    "\n",
    "Samples can be classified as predicting a passing score or a non-passing score by where they fall (above or below) on the line.  As a machine learning approach, we can provide a set of training data to create the model, then use future data to predict an outcome.\n",
    "\n",
    "**When to use:**\n",
    "+ Input data is quantitative.\n",
    "+ No colliniearity.\n",
    "+ No outliers.\n",
    "+ Relationship is expected to be linear\n",
    "\n",
    "**Additional Resources**\n",
    "+ [skLearn Logisitic Regression Function](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "+ [Wikipedia LR](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "+ [TowardsDataScience.com LR](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8)\n",
    "\n",
    "**Practice**\n",
    "\n",
    "To perform Logisitc Regression with sklearn, we must first create a LogisiticRegression object. There are two important arguments to provide:\n",
    "\n",
    "- `solver`: There are several different solvers: ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’.  These are meant for different types of data:\n",
    "   + For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
    "   + For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
    "   + ‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas ‘liblinear’ and ‘saga’ handle L1 penalty.\n",
    "- `multi_class`: set this argument when there are more than two classes in the outcome variable. Setting it to `auto` allows the algorithm to select the best approach for working with the data.  \n",
    "\n",
    "Because the iris species is multinomial (multiple categories), we will set the `multi_class` argument to `auto` and the `solver` to `libfgs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LogisticRegression object prepared for a multinomial outcome validation set.\n",
    "alg = LogisticRegression(solver='lbfgs', multi_class=\"auto\")\n",
    "\n",
    "# Execute the cross-validation strategy\n",
    "results['LogisticRegression'] = model_selection.cross_val_score(alg, Xt, Yt, cv=kfold, \n",
    "                                                                scoring=\"accuracy\", error_score=np.nan)\n",
    "\n",
    "# Take a look at the scores for each of the 10-fold runs.\n",
    "results['LogisticRegression']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Linear Discriminant Analysis (LDA)\n",
    "\n",
    "**Brief Summary** \n",
    "\n",
    "Linear Discriminant Analysis (LDA) is a classification method that employs a dimensionality reduction technique.  \n",
    "\n",
    "From the [Wikipedia LDA](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) page:\n",
    "\n",
    "> Linear discriminant analysis (LDA),... [is] a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events... LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label)\n",
    "\n",
    "Assumptions:\n",
    "+ Outcome classes are normally distributed.\n",
    "+ Variance between outcome classes is equal.\n",
    "\n",
    "LDA is similar to Principal Component Analysis (PCA), but….\n",
    "+ PCA is unsupervised, LDA is supervised\n",
    "+ PCA tries to maximize the variance.\n",
    "+ LDA tries to maximize the separation between data classes.\n",
    "\n",
    "<img src=\"./media/A08-lda_1.png\" style=\"height:250px\">\n",
    "<sup><i>Image from <a href=\"http://sebastianraschka.com/Articles/2014_python_lda.html\">SebastianRaschka Liniear Discriminant Analysis</a> page</i></sup>\n",
    "\n",
    "**When to use:**\n",
    "+ The dependent variable is categorical (qualitative) with \"classes\" or categories as outcomes \n",
    "+ The independnet variables are quantitative.\n",
    "\n",
    "**Additional Resources**\n",
    "+ [skLearn LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)\n",
    "+ [Wikipedia LDA](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)\n",
    "+ [TowardsDataSciecne LDA](https://towardsdatascience.com/classification-part-2-linear-discriminant-analysis-ea60c45b9ee5)\n",
    "\n",
    "**Practice**\n",
    "\n",
    "Similar to Logistic Regression, we must first create a LinearDiscriminantAnalysis object, then call `cross_val_score`.  Here we'll just use the default settings.  See the [online documentation](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) for a more thorough description of arguments and flexibility of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LinearDiscriminantAnalysis object with defaults.\n",
    "alg = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Execute the cross-validation strategy\n",
    "results['LinearDiscriminantAnalysis'] = model_selection.cross_val_score(alg, Xt, Yt, cv=kfold, \n",
    "                                                                        scoring=\"accuracy\", error_score=np.nan)\n",
    "\n",
    "# Take a look at the scores for each of the 10-fold runs.\n",
    "results['LinearDiscriminantAnalysis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 K-Nearest Neighbors (KNN)\n",
    "\n",
    "**Brief Summary**\n",
    "\n",
    "From the [Wikipedia KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) page:\n",
    "\n",
    "> In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression... \n",
    ">\n",
    "> <img src=\"./media/A08-KnnClassification.svg\" style=\"height: 200px\">\n",
    ">\n",
    "> [Above is an] example of k-NN classification. The test sample (green circle) should be classified either to the first class of blue squares or to the second class of red triangles. If k = 3 (solid line circle) it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the first class (3 squares vs. 2 triangles inside the outer circle).\n",
    "\n",
    "For the KNN approach to be useful, the outcome classes must be distinguishable. As an exapmle of distinguishable classes, consider the Iris dataset petal width vs petal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"petal_width\", y=\"petal_length\", hue=\"species\", data=iris);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Use**\n",
    "+ Multiple outcomes (not just binary)\n",
    "+ There are no assumptions for the outcome distribution\n",
    "+ The outcomes classes are already distinguishable in the problem space.\n",
    "\n",
    "**Additional Resources**\n",
    "+ [skLearn KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "+ [Wikipedia KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "+ [TowardsDataScience.com KNN](https://towardsdatascience.com/knn-k-nearest-neighbors-1-a4707b24bd1d)\n",
    "\n",
    "**Practice**\n",
    "\n",
    "Similar to other algorithms, we must first create a KNeighborsClassifier object, then call `cross_val_score`.  Here we'll just use the default settings.  See the [online documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for a more thorough description of arguments and flexibility of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KNeighborsClassifier object with defaults.\n",
    "alg = KNeighborsClassifier()\n",
    "\n",
    "# Execute the cross-validation strategy\n",
    "results['KNeighborsClassifier'] = model_selection.cross_val_score(alg, Xt, Yt, cv=kfold, \n",
    "                                                                  scoring=\"accuracy\", error_score=np.nan)\n",
    "# Take a look at the scores for each of the 10-fold runs.\n",
    "results['KNeighborsClassifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4  Classification and Regression Trees (Decision Trees)\n",
    "\n",
    "**Brief Summary** \n",
    "\n",
    "From the [Wikipedia Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning) page:\n",
    "\n",
    "> In computer science, Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\n",
    ">\n",
    ">![Decsion Trees](./media/A08-CART_tree_titanic_survivors.png)\n",
    ">\n",
    "> [The above] tree [shows] survival of passengers on the Titanic. The figures under the leaves show the probability of survival and the percentage of observations in the leaf.\n",
    "\n",
    "In short, trees are learned by splitting the source data based on an attribute value and two types of trees are possible based on the outcome data:\n",
    "\n",
    "Classification Trees\n",
    "- When outcome is categorical\n",
    "\n",
    "Regression Trees\n",
    "- When outcome is continuous\n",
    "\n",
    "**When to Use**\n",
    "- For both numerical and categorical outcome data\n",
    "- Distribution of outcome is unknown or doesn’t meet other model assumptions.\n",
    "- Multiple outcomes.\n",
    "\n",
    "**Additional Resources**\n",
    "- [sklearn Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [sklearn CART](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [Wikipediea Decision Trees](https://en.wikipedia.org/wiki/Decision_tree)\n",
    "- [TowardsDataScience.com CART](https://towardsdatascience.com/decision-trees-d07e0f420175)\n",
    "\n",
    "\n",
    "**Practice**\n",
    "\n",
    "Similar to other algorithms, we must first create a DecisionTreeClassifier object, then call `cross_val_score`.  Here we'll just use the default settings.  See the [online documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for a more thorough description of arguments and flexibility of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DecisionTreeClassifier object with defaults.\n",
    "alg = DecisionTreeClassifier()\n",
    "\n",
    "# Execute the cross-validation strategy\n",
    "results['DecisionTreeClassifier'] = model_selection.cross_val_score(alg, Xt, Yt, cv=kfold, \n",
    "                                                                  scoring=\"accuracy\", error_score=np.nan)\n",
    "# Take a look at the scores for each of the 10-fold runs.\n",
    "results['DecisionTreeClassifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5 Gaussian Naive Bayes (NB)\n",
    "\n",
    "**Brief Summary**\n",
    "\n",
    "From the [Wikipedia Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) page:\n",
    "\n",
    "> In machine learning, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n",
    "\n",
    "Naive Bayes methods rely on Bayes theorem, and follows axioms of the conditional probability:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A | B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "\\end{equation*}\n",
    "\n",
    ".\n",
    "Where:\n",
    " - A = outcome class\n",
    " - B = input data vector for a sample.\n",
    " \n",
    "There are many NB algorithms and they differ in assumptions about the distribution of P(A|B). For the Guassian approach the distributions are expected to be Guassian and the input data is quantitative.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "- High independence between input data (no multicollinearity)\n",
    "- Multiple outcome classes.\n",
    "- Independent variables are quantitative.\n",
    "\n",
    "**Additional Resources**\n",
    "- [skLearn Guassian NB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "- [Wikipedia NB](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "- [TowardsDataScience NB](https://towardsdatascience.com/all-about-naive-bayes-8e13cef044cf)\n",
    "\n",
    "**Practice**\n",
    "\n",
    "Similar to other algorithms, we must first create a GaussianNB object, then call `cross_val_score`.  Here we'll just use the default settings.  See the [online documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) for a more thorough description of arguments and flexibility of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GaussianNB object with defaults.\n",
    "alg = GaussianNB()\n",
    "\n",
    "# Execute the cross-validation strategy\n",
    "results['GaussianNB'] = model_selection.cross_val_score(alg, Xt, Yt, cv=kfold, \n",
    "                                                                  scoring=\"accuracy\", error_score=np.nan)\n",
    "# Take a look at the scores for each of the 10-fold runs.\n",
    "results['GaussianNB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.6 Support Vector Machines (SVM)\n",
    "**Brief Summary**\n",
    "\n",
    "A support vector machine attempts to find an optimal separation between outcomes by separating them in multi-dimensional space (&reals;<sup>n</sup>). Consider the following figure:\n",
    "\n",
    "<img src=\"./media/A08-Hyperplane.png\" style=\"height:250px\">\n",
    "<sup><i>Image from <a href=\"https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\">TowardsDataScience.com SVM</a> page</i></sup>\n",
    "\n",
    "Here the line in the first 2-dimensional plot separates the two classes and in the 3-dimensional plot, the plane separates them.  For larger dimensions, SVMs find the optimal **hyperplane**.  The optimal \"hyperplane\" (line or plane) is selected as the one which maximizes the **margin** between the outcome classes.  Consier the following figure:\n",
    "\n",
    "<img src=\"./media/A08-Svm_separating_hyperplanes.svg\" style=\"height:250px\">\n",
    "<sup><i>Image from <a href=\"https://en.wikipedia.org/wiki/Support-vector_machine\">Wikipedia SVM</a> page</i></sup>\n",
    "\n",
    "In the example figure above:\n",
    "- The H1 line does not separate the classes. \n",
    "- The H2 line does, but only with a small margin (distance between samples in the classes) \n",
    "- The H3 line separates them with the maximal margin.\n",
    "\n",
    "There are multiple types of SVMs.  The SVM that performs classification is known as a Support Vector Classification (SVC) algorithm.\n",
    "\n",
    "**When to use**\n",
    "- Multiple outcome classes.\n",
    "- Distribution of outcome is unknown or doesn’t meet other model assumptions.\n",
    "- High dimensional data\n",
    "- There are more samples than features (otherwise overfitting may occur)\n",
    "\n",
    "**Additional Resources**\n",
    "- [skLearn SVM](https://scikit-learn.org/stable/modules/svm.html)\n",
    "- [skLearn SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "- [Wikipedia SVM](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "- [TowardsDataScience.com SVM](https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f)\n",
    "\n",
    "\n",
    "**Practice**\n",
    "\n",
    "Similar to other algorithms, we must first create a SVC object, then call `cross_val_score`.  Here we'll just use the default settings.  See the [online documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) for a more thorough description of arguments and flexibility of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SVC object with defaults.\n",
    "alg = SVC(gamma='auto')\n",
    "\n",
    "# Execute the cross-validation strategy\n",
    "results['SVC'] = model_selection.cross_val_score(alg, Xt, Yt, cv=kfold, \n",
    "                                                                  scoring=\"accuracy\", error_score=np.nan)\n",
    "# Take a look at the scores for each of the 10-fold runs.\n",
    "results['SVC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Compare Scores\n",
    "Now that we have performed training and validation of six different methods we can compare the results to see which performed best.  We can do so by converting the results dictionary into a dataframe and using the `plot` function that comes with Panda's data frames to create a box plot of each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).plot(kind=\"box\", rot=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Test the model (make predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the boxplot in the previous section, it seems as if Linear Discriminant Analysis performed best!  We will use this algorithm and the trained model to compare predictions made from the 20% of data we set aside for testing match the actual outcomes.  To do this we will use two new functions that are members of any of the algorithm objects:\n",
    "\n",
    "- `fit`: This function employs the algorithm to create the final predictive model. All the training data is provided to this function. \n",
    "- `predict`:  This function uses the trained model to make predictions using the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LinearDiscriminantAnalysis object with defaults.\n",
    "alg = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Create a new model using all of the training data.\n",
    "alg.fit(Xt, Yt)\n",
    "\n",
    "# Using the testing data, predict the iris species.\n",
    "predictions = alg.predict(Xv)\n",
    "\n",
    "# Let's see the predictions\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaulate how accurate the model has been by comparing the predictions with the actual species for the test data.  The `accuracy_score` function provides this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Yv, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to explore the performance of the algorithm is by way of a **Confusion Matrix** or error matrix.  A confusion matrix is an <i>n</i> x <i>n</i> matrix where n is the number of outcome classes. In the case of the Iris data, <i>i</i> = 3.  To view the confusion matrix use the sklearn `confusion_matrix` function.  We pass in the testing outcome set, the predictions and the order that we want the classes to appear (using the `labels` argument):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['versicolor', 'virginica', 'setosa']\n",
    "cm = confusion_matrix(Yv, predictions, labels=labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements of the confusion matrix have the following meaning:\n",
    "- rows of the confusion matrix represent the predicted classes \n",
    "- columns represent the actual classes.  \n",
    "- elements on the diagnoal of the matrix represent the true positives\n",
    "- errors are present when counts above zero are outside of the diagnoal.\n",
    "\n",
    "***Note***: Because the result is a Numpy array, there are no row and column labels. However, because we provided the `labels` argument we know the order of the classes in the matrix.\n",
    "\n",
    "We can use the Seaborn `heatmap` to visualize the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, annot=True, xticklabels=labels, yticklabels=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `classification_report` function indicates how well the model was at predicting each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(Yv, predictions)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- `precision`: The precision is the ratio `tp / (tp + fp)` where `tp` is the number of true positives and `fp` the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "- `recall`: The recall is the ratio `tp / (tp + fn)` where `tp` is the number of true positives and `fn` the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "- `f1-score`: The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "- `support`: the number of actual samples with the given outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's examine a less performant model to see how the confusion matrix and reports indicate error.  Let's use the K-neighbors classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LinearDiscriminantAnalysis object with defaults.\n",
    "alg = KNeighborsClassifier()\n",
    "\n",
    "# Create a new model using all of the training data.\n",
    "alg.fit(Xt, Yt)\n",
    "\n",
    "# Using the testing data, predict the iris species.\n",
    "predictions = alg.predict(Xv)\n",
    "\n",
    "# Let's see the predictions\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Yv, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['versicolor', 'virginica', 'setosa']\n",
    "cm = confusion_matrix(Yv, predictions, labels=labels)\n",
    "sns.heatmap(cm, annot=True, xticklabels=labels, yticklabels=labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(Yv, predictions)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have one prediction that is false where `virginica` was predicted to be `versicolor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "Now that you have learned how to perform supervised machine learning using a variety of algorithms, lets practice using a new algorithm we haven't looked at yet: the Random Forest Classifier.  The random forest classifier builds multiple decision trees and merges them together.  Review the sklearn [online documentation for the RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).  For this task:\n",
    "\n",
    "1. Perform a 10-fold cross-validation strategy to see how well the random forest classifier performs with the iris data\n",
    "2. Use a boxplot to show the distribution of accuracy\n",
    "3. Use the `fit` and `predict` functions to see how well it performs with the testing data.\n",
    "4. Plot the confusion matrix\n",
    "5. Print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Outcomes\n",
    "At this point, you should feel comfortable with the following:\n",
    " - A basic understanding of popular supervised machine learning methods.\n",
    " - Preparing and normalizing data for supervised machine learning.\n",
    " - Splitting the data for testing and validation\n",
    " - k-fold strategy to training data.\n",
    " - Comparing scores\n",
    " - Testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Turn in?\n",
    "Be sure to **commit** and **push** your changes to this notebook.  All practice exercises should be completed.  Once completed, send a **Slack message** to the instructor indicating you have completed this assignment. The instructor will verify all work is completed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
