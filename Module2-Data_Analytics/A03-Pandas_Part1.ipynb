{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Pandas Part 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "This lesson meets the following learning objectives:\n",
    "- The ability to use Python data structures provided in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Read through all of the text in this page. This assignment provides step-by-step training divided into numbered sections. The sections often contain embeded exectable code for demonstration.  Section headers with icons have special meanings:  \n",
    "\n",
    "- <i class=\"fas fa-puzzle-piece\"></i> The puzzle icon indicates that the section provides a practice exercise that must be completed.  Follow the instructions for the exercise and do what it asks.  Exercises must be turned in for credit.\n",
    "- <i class=\"fa fa-cogs\"></i> The cogs icon indicates that the section provides a task to perform.  Follow the instructions to complete the task.  Tasks are not turned in for credit but must be completed to continue progress.\n",
    "\n",
    "Review the list of items in the **Expected Outcomes** section to check that you feel comfortable with the material you just learned. If you do not, then take some time to re-review that material again. If after re-review you are not comfortable, do not feel confident or do not understand the material, please ask questions on Slack to help.\n",
    "\n",
    "Follow the instructions in the **What to turn in** section to turn in the exercises of the assginment for course credit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This notebook is based on the official Pandas [documentation](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html).  Pandas is a Python package that we will use to manage labeled data. It has some similarities to R-like DataFrames.  Unless otherwise credited, quoted text comes from the official [10 Minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html#min) documentation which is an excellent resource, and provides the guide which we will drawn from for this tutorial.\n",
    "\n",
    "*Where does this tool fit in my life?*\n",
    "\n",
    "> For data scientists, working with data is typically divided into multiple stages: munging and cleaning data, analyzing / modeling it, then organizing the results of the analysis into a form suitable for plotting or tabular display. pandas is the ideal tool for all of these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Why use this package?*\n",
    "\n",
    "> pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Cheat Sheet\n",
    "\n",
    "Cheat sheets to remind you of popular Pandas functionality are available online.  You can find cheat sheets here:\n",
    "\n",
    "+ [Data Quest Pandas cheat sheet](./media/pandas-cheet-sheet.pdf)\n",
    "+ [Data Camp cheat sheet](./media/pandas-cheet-sheet.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-cogs\"></i> Notebook Setup\n",
    "\n",
    "First, we need to import the pandas library (and NumPy library too).  All packages are imported at the top of the notebook. Execute the code in the following cell to get started with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy and pandas usage are often intertwined.\n",
    "# These abbreviations are ubiquitiously used.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above imports both NumPy and Pandas as variables named `np` and `pd`. We can use these variables to access the functionality of both libraries respectively.  Use of `np` and `pd` is commonly used by many. You will find online tutorials and documentation that use this convention. Therefore, we will use those object names for the rest of this class.\n",
    "\n",
    "You may be wondering why we didn't import pandas like this:  \n",
    "```python\n",
    "import pandas\n",
    "```\n",
    "We could, but the first is far more commonly seen and the latter would require that we prefix all calls to pandas fuction with `pandas.`. It is easier to just prefix with `pd.`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Object Creation\n",
    "\n",
    "Here we will learn to create and use the two data structures provided by Pandas: **Series** and **DataFrames**.  For additional help, see the [Data Structure Intro section](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html#dsintro).\n",
    "\n",
    "\n",
    "In short, Pandas provides two new objects in which to store and manipulate data.\n",
    "\n",
    "Dimensions | Name |\tDescription\n",
    "-----------|-------|--------------------------------------\n",
    "1 |\tSeries | \t1D labeled homogeneously-typed array\n",
    "2 |\tDataFrame | \tGeneral 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column\n",
    "\n",
    "Typical usage of Pandas with data analysis will involve the creation of one or more `DataFrame` objects, and manipulating them with the provided methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The `pd.Series` Data Object\n",
    "\n",
    "In Pandas, a Series is a one dimensional array, and similar to NumPy, all the values are of the same data type (i.e. integer, string, boolean, etc.) or **NaN**.  NaN means \"not a number\" but serves as a marker when a value is missing.  \n",
    "\n",
    "Take a moment to review the [Series online documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html). DataFrames are Python objects, and as such they have member variables and functions. Take a moment to explore the member variables and functions listed on the left hand side of the page in the online documentation. Pick a few and see what they do.  Specifically, take a look at these member variables:  `.shape`, `.dtype`; and these functions: `.astype()`, `.min()`, and `.max()`.\n",
    "\n",
    "\n",
    "To begin, the following cell creates a new series containing an array of 5 numbers and one missing value by simply passing a list of 6 elements to the `pd.Series` function. Execute the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the series object.\n",
    "my_series = pd.Series([1, 3, 5, np.nan, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have introduced a missing values by using the NumPy variable `np.nan`. \n",
    "\n",
    "Execute the next cell to see what the new series object named `my_series` contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the object in the notebook.\n",
    "# This (mostly) calls the __repr__() function of a given object.\n",
    "my_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Create a series of your own design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4. The `pd.DataFrame` Data Object\n",
    "\n",
    "In Pandas, a DataFrame is a two-dimensional array.  Just like NumPy, it has axes labeled \"rows\" and \"cols\".  However, unlike NumPy, Pandas DataFrames have a few advantages over Numpy 2D matrices:  \n",
    "1. DataFrames allows for different data types in each column, where as NumPy arrays must always be of the same data type.\n",
    "2. DataFrames provide greater flexibilty for indexing and slicing.\n",
    "\n",
    "Take a moment to review the [DataFrames online documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). DataFrames are Python objects, and as such they have member variables and functions. Take a moment to explore the member variables and functions listed on the left hand side of the page in the online documentation. Pick a few and see what they do.  Specifically, take a look at these member variables:  `.shape`, `.dtypes`; and these functions: `.isna()`, `.duplicated()`, and `.nunique()`.\n",
    "\n",
    "#### 1.4.1 Create a DataFrame with a dictionary\n",
    "\n",
    "A dataframe is created by passing a Python 2D dictionary (i.e. **dict**) to the `pd.DataFrame` function.  Remember dictionaries have key/value pairs. The dictionary is interpreted in the following way:\n",
    "\n",
    "- The keys in the dictionary argument becomes the column names\n",
    "- The value in the dictionary argument contains the column values\n",
    "- Each \"column\" must have the same number of values.\n",
    "\n",
    "The following cell contains code that creates a new DataFrame object named `df_1`.  This dataframe consists of two columns, one named \"alpha\" and the other \"beta\".  Each column has 5 elements. Notice that the columns contain different data types.  Execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with a dictionary.\n",
    "df_1 = pd.DataFrame(\n",
    "    {'alpha': [0, 1, 2, 3, 4],\n",
    "     'beta': ['a', 'b', 'c', 'd', 'e']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell to see what the new data frame object named `df_1` contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the object in the notebook.\n",
    "# This time we get a html output, as the notebook shell\n",
    "# sees a _repr_html_() function to call.\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that a DataFrame can be visualized as a data table. Importantly, note that, a numerical index has been automatically generated and is shown in the left-most column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2. Create a DataFrame with a Numpy array\n",
    "Alternatively, we can create a DataFrame by passing in a Numpy matrix.  Remember that Numpy arrays can only store one data type, so if the data contains mixed types you would not use this approach. \n",
    "\n",
    "The following code creates a Dataframe of 6 rows and 4 columns and fills it with random numbers generated using the Numpy `np.random.randn` function.  Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our new data frame by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that because we did not provide a dictionary, we do not have header names for each column. Rather a numeric column index was automatically generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Create a pd.DataFrame object from a Python dictionary. Design the data as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Create a DataFrame with row and column labels\n",
    "\n",
    "Rather than have a numeric index for each row and column of the DataFrame, we may want to give each row a human-readable label. This will help make the data more accessible (as will be seen later).  To demonstrate, lets reuse the 6x4 datafame we created in section 2.2.2. Using this dataset we want to rename the row indexes with hypothetical dates that each row of information was collected, and we want to give the columns names, such as `A1`, `A2`, `B1` and `C1`.  \n",
    "\n",
    "First, let's set the row index names.  The code below uses the `pd.date_range` function to create a list of dates. Most likely, if we have dates in a real input dataset we would not be creating a list of dates, but for the purposes of demonstration we will generate a series of regularly spaces dates over a 6 day period: one for each of the 6 rows. Note that the `start` argument for the `pd.date_range` function expects the date in YYYYMMDD format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('20220301', periods=6)\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can re-create the DataFrame from section 2.2.2 with dates as row indexes and  `A1`, `A2`, `B1` and `C1` as column names. We do this by using the `index` and `columns` arguments when creating the DataFrame as shown in the following cell.  Execute the cell to see the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=('A1','A2', 'B1', 'C1'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Create a 10x5 dataframe of random numeric integers that follow a [Guassian (normal) Distribution](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html). \n",
    "  + Center the distrubtion at 0.85.\n",
    "  + We will use these values as assumed grades for a class of students\n",
    "+ Adjust the row indexes to be the names of hypothetical students.\n",
    "+ Adjust the columsn to be the names of hypothetical projects, homework, exam names, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "\n",
    "Creating a DataFrame manually, as in the previous examples, would be tedious for large datasets.  Fortunately, Pandas provides a variety of functions for importing data from files.  This saves you from writing your own Python code to open a file, read in the contents and format it for inclusiong into a DataFrame. For this tutorial we will look at two of these import functions:  `pd.read_csv` and `pd.read_excel`.\n",
    "\n",
    "For this portion of the tutorial, we will use a rather [famous set of data concerning iris flowers](https://en.wikipedia.org/wiki/Iris_flower_data_set).  You can find a copy of the comma-separated file accompanying this notebook in the `data` directory. You can view the file [here](./data/iris.csv).\n",
    "\n",
    "![Iris flower](./media/A03-193px-Iris_versicolor_3.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing Using `pd.read_csv`\n",
    "\n",
    "The `pd.read_csv` function can be used to read in files that are in comma-separated format or tab delimited format: two very common data formats.  This function is very flexible and has a large number of arguments. See the [`pd.read_csv` online documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) for a thorough listing of arguments. \n",
    "\n",
    "The iris data is in CSV format and is compatible with the default arguments for the `pd.read_csv` function, therefore we can easily import this data by providing the path to the file as the only argument to the function. This will automatically create a DataFrame for us! Execute the following line to import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Import the iris dataset.\n",
    "+ Take a look at the `pd.read_csv` online documentation. Write example code in a Markup cell for how you would import this file if it were tab-delimited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Importing using `pd.read_excel()`\n",
    "It is common for data to be stored in Excel worksheets (provided the data is not too large).  You can easily export data from Excel to CSV or Tab delimited formats but Pandas provides  the `pd.read_excel` function to save you the time.  We will not practice loading data from Excel, but take a look at the [`read_excel` online documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore your Data\n",
    "There are a variety of functions for exploring data once you have imported it. Here we will discuss `pd.head`, `pd.tail`, `pd.sample`, and `pd.describe`.\n",
    "\n",
    "### 3.1. The `pd.head` function\n",
    "\n",
    "As with any Python variable, you can display its contents in a Jupyter notebook or on the interactive Python terminal by simply typing the variable name and pressing the \"enter\" key.  However, for very large data files this may not be desired.  The data used in this tutorial is not extremly large but for large data we need better ways to examine the data.\n",
    "\n",
    "To easily examine the first set of rows in the DataFrame, use the `head` function of the `pd.DataFrame` object. By deafult it displays the first 5 rows. Execute the following cells to view the first five rows of the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an excellent way to check if you created your `pd.DataFrame` object correctly.  You can change the number of rows returned by providing an argument.  Note, in the table displayed above that the row index have been generated as numeric values but the column indexes are the names of the column headers from the input data file.  If the file has headers, the `pd.read_csv` function will automatically use those as column index names.\n",
    "\n",
    "\n",
    "The following requests the first 10 rows of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The `pd.tail` function\n",
    "Simliar to the `pd.head` function, the `pd.tail` function lets you peak at the last 5 rows of data.  To demonstrate, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The `pd.sample` function\n",
    "The `pd.sample` function selects a random set of rows to display. By default it only selects 1 row, we can provide the number of rows we would like (e.g. 5) by providing the number as an arugment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat execution of the function to see a different set of randomly selected rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the `sample` function can be very important beyond just viewing random rows. If you ever need to select a random sample of records from a dataset the `sample` function can do that for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The `pd.describe` function\n",
    "The `pd.describe` function helps demonstrate the power of Pandas over Python dictionaries and Numpy arrays.  It provides a summary of statistics for the data frame in one simple function call. These statistics include the count, mean, standard deviation, min, max and quartiles.\n",
    "\n",
    "Execute the following cell to demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    " + Use `head`, `tail` and `sample` with the iris dataset.\n",
    " + Do the same with the dataset you created in task 2c.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Accessing Data\n",
    "\n",
    "In this section we will learn to retrieve values, **slice** the data frame, and use column and row labels (rather than numeric indexes) to find data. \n",
    "\n",
    "### 4.1 Viewing column and row labels.\n",
    "Before we delve into retrieving data, lets first look at some properties of DataFrames for retrieving the column and row index names.  It is convenient to work with, or check the indexes and columns of a data frame using their names rather than a numeric index.  First, to retrieve the columns names use the `columns` property of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary, we can get a description of the index using the `index` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that here our index is numeric and runs from 0 to 150, incremented by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Display the columns and indexes of the iris dataset.\n",
    "+ Do the same with the dataset you created in Task 2c.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Get all the values\n",
    "\n",
    "Perhaps you would like to extract the contents of the DataFrame into a 2-dimensional Python list. You will lose the power of Pandas, but you should know how to do this. There are two ways to extract values: the newer method in Pandas versions above v0.24 way and the older way.  \n",
    "\n",
    "This following code example used to be the standard way of getting the values of a dataframe as a NumPy array:\n",
    "\n",
    "```python\n",
    "my_array = df.values\n",
    "```\n",
    "\n",
    "This is depreciated in version 0.24 of Pandas to instead require the use of `to_dict` function.\n",
    "\n",
    "```python\n",
    "my_array = df.to_dict()\n",
    "```\n",
    "\n",
    "You can verify what version of `pandas` you have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Check the version of `pandas` you have \n",
    "+ Use the appropriate method to convert the iris data to a dictionary.\n",
    "+ Do the same with the dataset you created in Task 2c.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Basic Selections\n",
    "Pandas provides a variety of row/column selectors that are similar to the way that lists and arrays are indxed in Python and NumPy. This tutorial will introduce those. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Get a column\n",
    "The easiest way to select a single column is to use the column label! When in doubt, use the `.column` property of the DataFrame to find the list of column labels.  Using the column label will result in the column being returned as a Series object.  For example, if we want the `sepal_length` column of the iris data frame we can execute the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slength = iris_df['sepal_length']\n",
    "slength.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe, we can use the `.head` function to get the first few rows of a Series as well as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Get rows (slicing)\n",
    "Similar to slicing of Python lists, we can do the same with Pandas DataFrames:\n",
    "\n",
    "```python\n",
    "iris_df[start:end]\n",
    "```\n",
    "\n",
    "As a reminder, indexing of a list in Python uses 0-based indexing and negative numbers index in reverse order:\n",
    "\n",
    "```\n",
    " +---+---+---+---+---+---+\n",
    " | P | y | t | h | o | n |\n",
    " +---+---+---+---+---+---+\n",
    "   0   1   2   3   4   5  \n",
    "  -6  -5  -4  -3  -2  -1\n",
    "```\n",
    "\n",
    "To practice, let's get the first row of the iris dataset by slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets get the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[0:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Selecting with `df.loc` and `df.iloc`\n",
    "\n",
    "Aside from the column name and slicing (as shown in the previous sections) Pandas provides the `df.loc` and `df.iloc` accessor.  These accessors perform better for selecting subsets of the dataframe. The `.loc` accessor is used to select rows and columns using index names and `.iloc` is used for selecting by numeric indexes.  Although, if your DataFrame only has numeric row indexes then `.loc` can still be used.\n",
    "\n",
    "However, please note:\n",
    "> While standard Python / NumPy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, '.at', '.iat', '.loc' and '.iloc'.\n",
    "\n",
    "In other words, rather than index Pandas arrays using something like, `iris[1:5,]`, instead use the functions that will be described below.  They are much faster and optimzied for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 Using `.loc`\n",
    "\n",
    "The `.loc` accessor allows for the slicing of the DataFrame by using the labels of either rows and columns.  \n",
    "\n",
    "The syntax is as follows:\n",
    "\n",
    "```python\n",
    "df.loc[row_start:row_end, column_start:column_end]\n",
    "```\n",
    "\n",
    "Remember, because the rows in the iris data frame are indexed using integers, we can use `.loc` to get rows using the integer values.  To get the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first two rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[0:1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that when only one row is returned that a Series is provided.  When multiple rows are returned then a new DataFrame object is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, recall that we constructed a 6x4 dataframe in section 2.3 that was filled with random values and we set the row names using datetime values. The DataFrame was named `df`.  As a reminder here are the first 2 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data frame the rows are indexed using strings that represent dates. We can therefore use those labels to slice the dataframe.  The following extracts the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2022-03-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can get the first two rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2022-03-01':'2022-03-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Using `.loc` to get columns\n",
    "Remember for our iris data frame the column indexes have the names as provided by the `.columns` property.\n",
    "\n",
    "```python\n",
    "iris_df.columns\n",
    "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
    "       'species'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these names to retrive values for specific columns. For example, the code below extracts rows 4 through 6 and the columns 'sepal_width' and 'petal_width':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[4:6, 'sepal_width': 'petal_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of the colon in the code above between 'sepal_width' and 'petal_width' implies a range. Therefore every column between those two are included. This is why 'petal_length' appears in the output.  If we wanted to specifically select only the two columns and none other, we could provide the names in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[4:6, ['sepal_width', 'petal_width']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that now we only have 'sepal_width', and 'petal_width' in the resulting data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can limit the rows to only those specified in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[[4,6], ['sepal_width', 'petal_width']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "+ Use any iris dataframe to:\n",
    "  + Select a row slice with `loc`.\n",
    "  + Select a row and column slice with `loc`.\n",
    "  + Take a look at the [Pandas documentation for the `at` selector](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.at.html). Use what you learn there to select a single item with Pandas `at` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4. Using `.iloc`\n",
    "\n",
    "The `.iloc` accessor allows us to slice the data frame using an integer index, regardless of what the index (or column) labels actually are.\n",
    "\n",
    "The syntax is the same as `.loc` with the exception of that integers are used instead of labels:\n",
    "\n",
    "```python\n",
    "df.iloc[row_start:row_end, column_start:column_end]\n",
    "```\n",
    "To retrieve the first row of the iris data we use the numeric index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.iloc[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use any combination of integer indexes to get a subset of rows and columns.  The following examples retrieves rows 2 to 5, and excludes the 'species' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.iloc[2:5, :-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.5.<i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "\n",
    "+ Use any iris dataframe to:    \n",
    "    + Select a row slice with `iloc`.\n",
    "    + Select a row and column slice with `iloc`.\n",
    "    + Take a look at the [Pandas documentation for the `iat` selector](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iat.html). Use what you learn there to select a single item with Pandas `iat` accessor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Boolean Indexing\n",
    "\n",
    "Pandas allows us to use boolean values to extract specific rows of data that meet certain conditions.To clarify this, lets first examine what occurs when we apply a condition to a data frame.  Create a new dataframe with the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame(\n",
    "    {'alpha': [0, 1, 2, 3, 4],\n",
    "     'beta': [5, 6, 7, 8, 9]})\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a condition to the data frame to find all values > 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 > 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resulting data frame contains `True` and `False` values in place of every value that did or did not meet the condition.  We can use this new data frame to subset the original dataframe.\n",
    "\n",
    "Lets consider the iris dataset for a more realistic example.  Suppose we want to find all rows with a `sepal_length` greater than 5.8. We know from the call to `describe` that the mean is approximately 5.8. First, we will identify rows that meet our criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = iris_df['sepal_length'] > 5.8\n",
    "condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Series `condition` indicates `False` if the sepal_length value in the row is <= 5.8 and `True` if it is > 5.8. We can use this array to slice the rows of the dataframe in the following way (only rows with a `True` in the `condition` object are returned):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[condition].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could rewrite the lines above to combine them into a single line with identical results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[iris_df['sepal_length'] > 5.8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. <i class=\"fas fa-puzzle-piece\"></i> Practice\n",
    "\n",
    "In the cell below notebook, perform the following.\n",
    "\n",
    "\n",
    "+ Create subsets of the iris dataset using boolean indexes that:\n",
    "    + Use one boolean operator.\n",
    "    + Use two boolean operators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Outcomes\n",
    "At this point, you should feel comfortable with the following:\n",
    "- What is a Pandas series.\n",
    "- What is a Pandas dataframe.\n",
    "- Importing files with Pandas.\n",
    "- Exploring data with dataframe functions.\n",
    "- Accession values in dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Turn in?\n",
    "Be sure to **commit** and **push** your changes to this notebook.  All practice exercises should be completed.  Once completed, send a **Slack message** to the instructor indicating you have completed this assignment. The instructor will verify all work is completed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
